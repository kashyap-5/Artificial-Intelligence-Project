# -*- coding: utf-8 -*-
"""AI Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SHSfTdVuotJEq6SrnmvYpHbAb0-4sQfR
"""

import statistics
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Conv2D
from tensorflow.keras.datasets import fashion_mnist
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import activations
from keras.layers import GlobalAveragePooling2D, BatchNormalization

(X_train,Y_train),(X_test,Y_test)=fashion_mnist.load_data()
print(X_train.shape)
print(Y_train.shape)
print(X_test.shape)
print(Y_test.shape)

X_train=X_train/255
X_test=X_test/255

plt.imshow(X_train[3])

a, b = plt.subplots(3,3) 
a.set_size_inches(6, 6)
k = 0
for i in range(3):
    for j in range(3):
        b[i,j].imshow(X_train[k].reshape(28, 28))
        k += 1
    plt.tight_layout()

X_train=np.array(X_train).reshape(-1,28,28,1)
X_test=np.array(X_test).reshape(-1,28,28,1)
print(X_train.shape)

## define the list of types of activation function
activ_func= ['tanh','relu','sigmoid','gelu','swish']

## define the list of the filter
filter=[4]
for i in range(8,130,8):
    filter.append(i)
print((filter))

## define the list of kernal size
kernal_size=[1,2,3,4,5,6,7]

## define function for the genome code 
def genome(activ_func_FL_layer,activ_func_NC_layer,activ_func_RC_layer,kernal_size_NC_layer,kernal_size_RC_layer,NC_filter,RC_filter):
    gen = "NC "+str(NC_filter)+" "+str(kernal_size_NC_layer)+" "+str(activ_func_NC_layer)+";"+"RC "+str(RC_filter)+" "+str(kernal_size_RC_layer)+" "+str(activ_func_RC_layer)+";"+"NC "+str(NC_filter)+" "+str(kernal_size_NC_layer)+" "+str(activ_func_NC_layer)+";"+"RC "+str(RC_filter)+" "+str(kernal_size_RC_layer)+" "+str(activ_func_RC_layer)+";"+"FL "+str(activ_func_FL_layer)+";"
    return gen

## function to fit the model
def Dfit(model):
    model.summary()
    model.compile(loss="sparse_categorical_crossentropy",optimizer="adam",metrics=['accuracy'])
    history=model.fit(X_train,Y_train,epochs=10,validation_split=0.2)
    return model.evaluate(X_train,Y_train)[1]*100 , model.evaluate(X_test,Y_test)[1]*100

## function to calculate the parameters
def paramt(model):
    tparam=np.sum([np.prod(v.get_shape()) for v in model.trainable_weights])
    ntparam=np.sum([np.prod(v.get_shape()) for v in model.non_trainable_weights])
    return tparam+ntparam

## function to print the data
def printData(gen,param,train_acc,test_acc):
    print("#############################################################################################")
#    print("the model is                      :",  )
    print("the genome code is                :",  gen)
    print("the total number of parameters are:", param)
    print("the training accuracy is          :",train_acc)
    print("the testing accuracy is           :",test_acc)
    print("#############################################################################################")

## create function to train the model
content=[]
import random
def train_model():
    
    activ_func_NC_layer= random.choice(activ_func)
    activ_func_RC_layer=random.choice(activ_func)
    activ_func_FL_layer=random.choice(activ_func)
    
    kernal_size_NC_layer =random.choice(kernal_size)
    kernal_size_RC_layer =random.choice(kernal_size)
    
    NC_filter=random.choice(filter)
    RC_filter=random.choice(filter)
    
    model=tf.keras.Sequential()
    model.add(Conv2D(NC_filter , (kernal_size_NC_layer,kernal_size_NC_layer) , strides = (1,1) , padding = 'same' , activation = activ_func_NC_layer , input_shape = X_train.shape[1:]))
    model.add(BatchNormalization())
    model.add(Conv2D(RC_filter , (kernal_size_RC_layer,kernal_size_RC_layer) , strides = (2,2) , padding = 'valid' , activation = activ_func_RC_layer ))
    model.add(BatchNormalization())
    model.add(Conv2D(NC_filter , (kernal_size_NC_layer,kernal_size_NC_layer) , strides = (1,1) , padding = 'same' , activation = activ_func_NC_layer ))
    model.add(BatchNormalization())
    model.add(Conv2D(RC_filter , (kernal_size_RC_layer,kernal_size_RC_layer) , strides = (2,2) , padding = 'valid' , activation = activ_func_RC_layer ))
    model.add(BatchNormalization())
    
    model.add(GlobalAveragePooling2D())
    model.add(layers.Dense(64, activation=activ_func_FL_layer))
    model.add(layers.Dense(10, activation=activations.softmax))
    model.summary()
    
    ##fit the data
    train_acc,test_acc=Dfit(model)
    ## finding genome string
    gen=genome(activ_func_FL_layer,activ_func_NC_layer,activ_func_RC_layer,kernal_size_NC_layer,kernal_size_RC_layer,NC_filter,RC_filter)
    ##find total parameters
    param=paramt(model)
    ## print 
    printData(gen,param,train_acc,test_acc)
    
    ##store the data in csv file
    cont={
        "genome string":gen,
        "parameters":param,
        "training accuracy":train_acc,
        "testing accuracy":test_acc
    }
    content.append(cont)

## loop for different combinations
## for just the showing purpose i'm taking 1 value inside the for loop but it denotes that how many models you want to make.
## ihave made 120 models and stored in the csv file
for i in range(1):
    try:
        train_model()
    except:
        pass

import pandas as pd
dfff=pd.DataFrame(content)
dfff.to_csv("models_data.csv")

content

dfff

"""TILL NOW WE HAVE CREATED THE CNN MODEL AND TRAINED DIFFERENT MODELS AND SAVED THEM INTO CSV FILES."""

df=pd.read_csv('C:/Users/NIKHIL/Desktop/AI.csv')
df

data = df.sort_values(by=['parameters'])
data

data_test_acc=data.iloc[:,4]
data_para=data.iloc[:,2]

## see plot between parameters versus testing accuracy
plt.plot(data_para,data_test_acc)
plt.rcParams["figure.figsize"] = (8,4)

X = data.iloc[:,2]
Y = data.iloc[:,4]
d_gen=data.iloc[:,1]

X = X.values
Y = Y.values

mean_X = statistics.mean(X)
mean_X

X_l = []
for i in X:
  if i>244924:
    break
  X_l.append(i)

X_l_mean  = statistics.mean(X_l)
X_l_mean

X0 = random.choice(X_l)
pos = np.where(X==X0)
intial_val = Y[pos]
print(pos[0][0],intial_val)

def starting_point():
  X0 = random.choice(X_l)
  pos = np.where(X==X0)
  intial_val = Y[pos]
  return pos[0][0],intial_val

def difference_ratio(current_param,current_acc,best_parm,best_acc):
  temp1 = current_param-best_parm
  temp2 = current_acc-best_acc
  ratio = temp1/temp2
  return ratio

pos,intial_val = starting_point()
count = 0
while(1):
  X0_left_1 = pos -1
  X0_right_1 = pos + 1
  X0_left_2 = pos -2
  X0_right_2 = pos + 2

  if(Y[X0_left_1]>intial_val):
    final_seq = X0_left_1
    #print("Taking Left 1",X[X0_left_1],Y[X0_left_1])
    pos = X0_left_1
  
  elif(Y[X0_right_1]>intial_val):
    current_param = X[X0_right_1]
    current_acc = Y[X0_right_1]
    best_parm = X[pos]
    best_acc = Y[pos]
    ratio = difference_ratio(current_param,current_acc,best_parm,best_acc)
    if ratio <100000:
      final_seq = X0_right_1
      #print("Taking Right 1",X[X0_right_1],Y[X0_right_1])
    pos = X0_right_1
    
  
  elif(Y[X0_left_2]>intial_val):
    final_seq = X0_left_2
    #print("Taking Left 2",X[X0_left_2],Y[X0_left_2])
    pos = X0_left_2
  
  elif(Y[X0_right_2]>intial_val):
    current_param = X[X0_right_1]
    current_acc = Y[X0_right_1]
    best_parm = X[pos]
    best_acc = Y[pos]
    ratio = difference_ratio(current_param,current_acc,best_parm,best_acc)
    if ratio <150000:
      final_seq = X0_right_2
      #print("Taking Right 2",X[X0_right_2],Y[X0_right_2])
    pos = X0_right_2

  if(count==50):
    if Y[pos]>75.0:
      final_seq = pos
      #print("Taking ",pos,Y[pos])
      break
    else:
      count=0
  
  count+=1
  pos,intial_val = starting_point()
  #print("Best not found",pos,intial_val,count)

pos

print("The testing accuracy of the best model is:",Y[pos])

print("The respective parameters are :",X[pos])

gen=data.iloc[pos,1]
data.iloc[pos]

gen

NC_layer1,RC_layer1,NC_layer2,RC_layer2,FL_layer,sdd=gen.split(';')
type(NC_layer1)
sddd,NC_filter,kernal_size_NC_layer,activ_func_NC_layer=NC_layer1.split(' ')
print(type(int(NC_filter)))

"""TILL NOW WE HAVE APPLIED HILL CLIMBING AND FOUND THE BEST MODEL FOR US."""

## function to break the genome
def break_genome(gen):
    NC_layer1,RC_layer1,NC_layer2,RC_layer2,FL_layer,sdd=gen.split(';')
    
    #break NC_layer
    sddd,NC_filter,kernal_size_NC_layer,activ_func_NC_layer=NC_layer1.split(' ')
    NC_filter=int(NC_filter)
    kernal_size_NC_layer=int(kernal_size_NC_layer)
    
    #break RC layer
    sdddd,RC_filter,kernal_size_RC_layer,activ_func_RC_layer=RC_layer1.split(' ')
    RC_filter=int(RC_filter)
    kernal_size_RC_layer=int(kernal_size_RC_layer)
    
    #break full layer
    sd,activ_func_FL_layer=FL_layer.split(' ')
    
    best_model(NC_filter,RC_filter,kernal_size_NC_layer,kernal_size_RC_layer,activ_func_NC_layer,activ_func_RC_layer,activ_func_FL_layer)

## function to train the best model we get after searching
def best_model(NC_filter,RC_filter,kernal_size_NC_layer,kernal_size_RC_layer,activ_func_NC_layer,activ_func_RC_layer,activ_func_FL_layer):
    model=tf.keras.Sequential()
    model.add(Conv2D(NC_filter , (kernal_size_NC_layer,kernal_size_NC_layer) , strides = (1,1) , padding = 'same' , activation = activ_func_NC_layer , input_shape = X_train.shape[1:]))
    model.add(BatchNormalization())
    model.add(Conv2D(RC_filter , (kernal_size_RC_layer,kernal_size_RC_layer) , strides = (2,2) , padding = 'valid' , activation = activ_func_RC_layer ))
    model.add(BatchNormalization())
    model.add(Conv2D(NC_filter , (kernal_size_NC_layer,kernal_size_NC_layer) , strides = (1,1) , padding = 'same' , activation = activ_func_NC_layer ))
    model.add(BatchNormalization())
    model.add(Conv2D(RC_filter , (kernal_size_RC_layer,kernal_size_RC_layer) , strides = (2,2) , padding = 'valid' , activation = activ_func_RC_layer ))
    model.add(BatchNormalization())
    
    model.add(GlobalAveragePooling2D())
    model.add(layers.Dense(64, activation=activ_func_FL_layer))
    model.add(layers.Dense(10, activation=activations.softmax))
    model.summary()
    
    model.compile(loss="sparse_categorical_crossentropy",optimizer="adam",metrics=['accuracy'])
    history=model.fit(X_train,Y_train,epochs=10,validation_split=0.2)
    
    ## plot the accuracy and loss of both the training data and validation data
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']

    loss = history.history['loss']
    val_loss = history.history['val_loss']
    plt.subplot(1, 2, 1)
    plt.plot(range(10), acc, label='Training Accuracy')
    plt.plot(range(10), val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Accuracy')

    plt.subplot(1, 2, 2)
    plt.plot(range(10), loss, label='Training Loss')
    plt.plot(range(10), val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Loss')
    plt.show()

break_genome(gen)



